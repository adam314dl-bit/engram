# =============================================================================
# Engram Production Configuration (BM25 + Graph Mode)
# =============================================================================

# LLM Configuration (remote OpenAI-compatible endpoint)
LLM_BASE_URL=http://your-host:8888/v1
LLM_MODEL=kimi-k2
LLM_API_KEY=your-api-key

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=engram2024

# =============================================================================
# Retrieval Mode
# =============================================================================
# bm25_graph = BM25 + Graph only (no embeddings, faster startup/ingestion)
# hybrid = BM25 + Graph + Vector search (requires embedding model)
RETRIEVAL_MODE=bm25_graph

# =============================================================================
# RRF Fusion Weights (v4.5)
# =============================================================================
RRF_K=60
RRF_BM25_WEIGHT=0.35
RRF_GRAPH_WEIGHT=0.30
RRF_PATH_WEIGHT=0.35

# =============================================================================
# Retrieval Parameters
# =============================================================================
RETRIEVAL_TOP_K=100
RETRIEVAL_BM25_K=200

# =============================================================================
# Path-Based Retrieval (v4.5)
# =============================================================================
PATH_MAX_LENGTH=3
PATH_BRIDGE_MAX_HOPS=2
PATH_SHARED_MIN_LINKS=2
PATH_MEMORY_LIMIT=50

# =============================================================================
# Reranker (BGE Reranker v2-m3 - better Russian support)
# =============================================================================
RERANKER_ENABLED=true
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_CANDIDATES=64
RERANKER_DEVICE=cuda:0
RERANKER_USE_FP16=true

# =============================================================================
# BM25 Parameters
# =============================================================================
BM25_LEMMATIZE=true
BM25_REMOVE_STOPWORDS=true

# =============================================================================
# Spreading Activation
# =============================================================================
ACTIVATION_DECAY=0.85
ACTIVATION_THRESHOLD=0.3
ACTIVATION_MAX_HOPS=3
ACTIVATION_RESCALE=0.4
SEMANTIC_EDGE_BOOST=1.5

# =============================================================================
# Parallelism
# =============================================================================
INGESTION_MAX_CONCURRENT=32
LLM_MAX_CONCURRENT=32
LLM_TIMEOUT=300.0

# =============================================================================
# Enrichment LLM (v4.4 - for graph quality optimization)
# =============================================================================
# Production: Qwen3-4B on vLLM Docker
# docker run -d --name engram-enrichment --runtime nvidia --gpus '"device=1"' \
#   -v /data/cache/huggingface:/root/.cache/huggingface -p 8889:8000 --ipc=host \
#   --restart unless-stopped vllm/vllm-openai:latest --model Qwen/Qwen3-4B \
#   --gpu-memory-utilization 0.3 --max-model-len 8192 --dtype bfloat16
ENRICHMENT_LLM_ENABLED=true
ENRICHMENT_LLM_BASE_URL=http://localhost:8889/v1
ENRICHMENT_LLM_MODEL=Qwen/Qwen3-4B
ENRICHMENT_LLM_TIMEOUT=30.0
ENRICHMENT_LLM_MAX_CONCURRENT=128

# =============================================================================
# Observability (v4.5)
# =============================================================================
OBSERVABILITY_ENABLED=false
OBSERVABILITY_SAVE_TRACES=false
OBSERVABILITY_TRACE_DIR=./traces

# =============================================================================
# Consolidation Parameters
# =============================================================================
CONSOLIDATION_MIN_REPETITIONS=3
CONSOLIDATION_MIN_SUCCESS_RATE=0.85
CONSOLIDATION_MIN_IMPORTANCE=7.0
REFLECTION_IMPORTANCE_THRESHOLD=150.0

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false

# =============================================================================
# Optional: Hybrid Mode (uncomment to enable vector search)
# =============================================================================
# RETRIEVAL_MODE=hybrid
# RETRIEVAL_VECTOR_K=200
# RRF_VECTOR_WEIGHT=0.35
# EMBEDDING_MODEL=ai-sage/Giga-Embeddings-instruct
# EMBEDDING_DIMENSIONS=1024
# EMBEDDING_BATCH_SIZE=128
