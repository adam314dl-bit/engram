# =============================================================================
# Engram Production Configuration
# =============================================================================

# LLM Configuration (remote OpenAI-compatible endpoint)
LLM_BASE_URL=http://your-host:8888/v1
LLM_MODEL=kimi-k2
LLM_API_KEY=your-api-key

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=engram2024

# Embeddings (local HuggingFace model - Russian optimized)
EMBEDDING_MODEL=ai-sage/Giga-Embeddings-instruct
EMBEDDING_DIMENSIONS=1024
EMBEDDING_BATCH_SIZE=128
EMBEDDING_MULTI_GPU=false
EMBEDDING_GPU_COUNT=1

# Parallelism (tune based on vLLM capacity)
INGESTION_MAX_CONCURRENT=8
LLM_MAX_CONCURRENT=8
LLM_TIMEOUT=300.0

# Spreading Activation Parameters
ACTIVATION_DECAY=0.85
ACTIVATION_THRESHOLD=0.3
ACTIVATION_MAX_HOPS=3
ACTIVATION_RESCALE=0.4

# Consolidation Parameters
CONSOLIDATION_MIN_REPETITIONS=3
CONSOLIDATION_MIN_SUCCESS_RATE=0.85
CONSOLIDATION_MIN_IMPORTANCE=7.0
REFLECTION_IMPORTANCE_THRESHOLD=150.0

# Retrieval Parameters
RETRIEVAL_TOP_K=10
RETRIEVAL_BM25_K=20
RETRIEVAL_VECTOR_K=20

# BM25 Parameters
BM25_LEMMATIZE=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false

# v4.3 Query Enrichment
QUERY_ENRICHMENT_ENABLED=true

# v4.3.1 Enrichment LLM (fast model for query enrichment)
# Production: Qwen3-4B on vLLM Docker (GPU 1)
# docker run -d --name engram-enrichment --runtime nvidia --gpus '"device=1"' \
#   -v /data/cache/huggingface:/root/.cache/huggingface -p 8889:8000 --ipc=host \
#   --restart unless-stopped vllm/vllm-openai:latest --model Qwen/Qwen3-4B \
#   --gpu-memory-utilization 0.3 --max-model-len 8192 --dtype bfloat16
ENRICHMENT_LLM_ENABLED=true
ENRICHMENT_LLM_BASE_URL=http://localhost:8889/v1
ENRICHMENT_LLM_MODEL=Qwen/Qwen3-4B
ENRICHMENT_LLM_TIMEOUT=30.0
ENRICHMENT_LLM_MAX_CONCURRENT=24
