# =============================================================================
# Engram Production Configuration
# =============================================================================

# LLM (remote OpenAI-compatible endpoint)
LLM_BASE_URL=http://your-host:8888/v1
LLM_MODEL=kimi-k2
LLM_API_KEY=your-api-key

# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=engram2024

# =============================================================================
# Retrieval (Hybrid Mode - 4-way fusion)
# =============================================================================
RETRIEVAL_MODE=hybrid
RETRIEVAL_TOP_K=100
RETRIEVAL_BM25_K=200
RETRIEVAL_VECTOR_K=200

# RRF Fusion (4-way weights, sum to 1.0)
RRF_K=60
RRF_BM25_WEIGHT=0.25
RRF_VECTOR_WEIGHT=0.25
RRF_GRAPH_WEIGHT=0.20
RRF_PATH_WEIGHT=0.30

# Path-Based Retrieval
PATH_MAX_LENGTH=3
PATH_BRIDGE_MAX_HOPS=2
PATH_SHARED_MIN_LINKS=2
PATH_MEMORY_LIMIT=50

# =============================================================================
# Vector Search (BGE-M3)
# =============================================================================
BGE_MODEL_NAME=BAAI/bge-m3
BGE_USE_FP16=true
BGE_BATCH_SIZE=32
VECTOR_INDEX_PATH=./data/vector_index

# =============================================================================
# Reranker (BGE - better Russian support)
# =============================================================================
RERANKER_ENABLED=true
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_CANDIDATES=64
RERANKER_DEVICE=cuda:0
RERANKER_USE_FP16=true

# =============================================================================
# BM25 (Russian NLP)
# =============================================================================
BM25_LEMMATIZE=true
BM25_REMOVE_STOPWORDS=true

# =============================================================================
# Spreading Activation
# =============================================================================
ACTIVATION_DECAY=0.85
ACTIVATION_THRESHOLD=0.3
ACTIVATION_MAX_HOPS=3
ACTIVATION_RESCALE=0.4
SEMANTIC_EDGE_BOOST=1.5

# =============================================================================
# Parallelism
# =============================================================================
INGESTION_MAX_CONCURRENT=32
LLM_MAX_CONCURRENT=32
LLM_TIMEOUT=300.0

# =============================================================================
# Enrichment LLM (Qwen3-4B on vLLM, see CLAUDE.md for Docker setup)
# =============================================================================
ENRICHMENT_LLM_ENABLED=true
ENRICHMENT_LLM_BASE_URL=http://localhost:8889/v1
ENRICHMENT_LLM_MODEL=Qwen/Qwen3-4B
ENRICHMENT_LLM_TIMEOUT=30.0
ENRICHMENT_LLM_MAX_CONCURRENT=128

# =============================================================================
# API
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Optional: BM25+Graph Mode (no vector search, faster startup)
# =============================================================================
# RETRIEVAL_MODE=bm25_graph
# RRF_BM25_WEIGHT=0.35
# RRF_GRAPH_WEIGHT=0.30
# RRF_PATH_WEIGHT=0.35
