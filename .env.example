# =============================================================================
# Engram Configuration
# =============================================================================

# LLM Configuration (remote OpenAI-compatible endpoint)
LLM_BASE_URL=http://localhost:11434/v1
LLM_MODEL=qwen3:8b
LLM_API_KEY=ollama
LLM_MAX_CONCURRENT=16

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=engram_password

# Embeddings (local HuggingFace model)
# Dev: all-MiniLM-L6-v2 (384 dims)
# Prod: ai-sage/Giga-Embeddings-instruct (1024 dims)
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSIONS=384

# Spreading Activation Parameters
ACTIVATION_DECAY=0.85
ACTIVATION_THRESHOLD=0.3
ACTIVATION_MAX_HOPS=3
ACTIVATION_RESCALE=0.4

# Consolidation Parameters
CONSOLIDATION_MIN_REPETITIONS=3
CONSOLIDATION_MIN_SUCCESS_RATE=0.85
CONSOLIDATION_MIN_IMPORTANCE=7.0
REFLECTION_IMPORTANCE_THRESHOLD=150.0

# Retrieval Parameters
RETRIEVAL_TOP_K=10
RETRIEVAL_BM25_K=20
RETRIEVAL_VECTOR_K=20

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false
